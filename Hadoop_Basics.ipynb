{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## commands: dkstart dkstop dkcreate starthdfs stophdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "bigred@CVBG:~/wk$ dkstart a\n",
    "cla00 starting\n",
    "  java version \"1.8.0_101\"\n",
    "  Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n",
    "  CVBG:22100->172.17.2.10:22\n",
    "\n",
    "cla01 starting\n",
    "^C\n",
    "bigred@CVBG:~/wk$ dkstop a\n",
    "cla00 Exiting\n",
    "CVBG:22100->172.17.2.10:22 deleted\n",
    "\n",
    "cla01 Exiting\n",
    "\n",
    "nna does not running\n",
    "rma does not running\n",
    "wka01 does not running\n",
    "wka02 does not running\n",
    "wka03 does not exist\n",
    "bigred@CVBG:~/wk$ dkcreate a\n",
    "yes/no : yes\n",
    "\n",
    "cla00 exist\n",
    "cla01 exist\n",
    "nna exist\n",
    "rma exist\n",
    "wka01 exist\n",
    "wka02 exist\n",
    "wka03 created\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "bigred@CVBG:~/wk$ starthdfs a\n",
    "Cluster Type = small\n",
    "starting namenode, logging to /tmp/hadoop-bigred-namenode-nna.out\n",
    "starting secondarynamenode, logging to /tmp/hadoop-bigred-secondarynamenode-nna.out\n",
    "starting datanode, logging to /tmp/hadoop-bigred-datanode-wka01.out\n",
    "starting datanode, logging to /tmp/hadoop-bigred-datanode-wka02.out\n",
    "wka03 not running\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "bigred@CVBG:~/wk$ stophdfs a\n",
    "stopping namenode\n",
    "dksstopping secondarynamenode\n",
    "topstopping datanode\n",
    "stopping datanode\n",
    "no datanode to stop\n",
    "bigred@CVBG:~/wk$ dkstop a\n",
    "cla00 Exiting\n",
    "CVBG:22100->172.17.2.10:22 deleted\n",
    "\n",
    "cla01 Exiting\n",
    "CVBG:22101->172.17.2.11:22 deleted\n",
    "\n",
    "nna Exiting\n",
    "rma Exiting\n",
    "wka01 Exiting\n",
    "wka02 Exiting\n",
    "wka03 Exiting\n",
    "bigred@CVBG:~/wk$ dkstart a\n",
    "cla00 starting\n",
    "  java version \"1.8.0_101\"\n",
    "  Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n",
    "  CVBG:22100->172.17.2.10:22\n",
    "\n",
    "cla01 starting\n",
    "  java version \"1.8.0_101\"\n",
    "  Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n",
    "  CVBG:22101->172.17.2.11:22\n",
    "\n",
    "nna starting\n",
    "  java version \"1.8.0_101\"\n",
    "  Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n",
    "\n",
    "rma starting\n",
    "  java version \"1.8.0_101\"\n",
    "  Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n",
    "\n",
    "wka01 starting\n",
    "  java version \"1.8.0_101\"\n",
    "  Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n",
    "\n",
    "wka02 starting\n",
    "  java version \"1.8.0_101\"\n",
    "  Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n",
    "\n",
    "wka03 starting\n",
    "  java version \"1.8.0_101\"\n",
    "  Scala compiler version 2.9.2 -- Copyright 2002-2011, LAMP/EPFL\n",
    "\n",
    "bigred@CVBG:~/wk$ starthdfs a\n",
    "Cluster Type = small\n",
    "starting namenode, logging to /tmp/hadoop-bigred-namenode-nna.out\n",
    "starting secondarynamenode, logging to /tmp/hadoop-bigred-secondarynamenode-nna.out\n",
    "starting datanode, logging to /tmp/hadoop-bigred-datanode-wka01.out\n",
    "starting datanode, logging to /tmp/hadoop-bigred-datanode-wka02.out\n",
    "starting datanode, logging to /tmp/hadoop-bigred-datanode-wka03.out\n",
    "bigred@CVBG:~/wk$ hdfs dfsadmin -report\n",
    "Configured Capacity: 177152446464 (164.99 GB)\n",
    "Present Capacity: 144329437184 (134.42 GB)\n",
    "DFS Remaining: 144329355264 (134.42 GB)\n",
    "DFS Used: 81920 (80 KB)\n",
    "DFS Used%: 0.00%\n",
    "Under replicated blocks: 0\n",
    "Blocks with corrupt replicas: 0\n",
    "Missing blocks: 0\n",
    "Missing blocks (with replication factor 1): 0\n",
    "\n",
    "-------------------------------------------------\n",
    "Live datanodes (3):\n",
    "\n",
    "Name: 172.17.8.11:50010 (wka02)\n",
    "Hostname: wka02\n",
    "Decommission Status : Normal\n",
    "Configured Capacity: 59050815488 (55.00 GB)\n",
    "DFS Used: 28672 (28 KB)\n",
    "Non DFS Used: 10941001728 (10.19 GB)\n",
    "DFS Remaining: 48109785088 (44.81 GB)\n",
    "DFS Used%: 0.00%\n",
    "DFS Remaining%: 81.47%\n",
    "Configured Cache Capacity: 0 (0 B)\n",
    "Cache Used: 0 (0 B)\n",
    "Cache Remaining: 0 (0 B)\n",
    "Cache Used%: 100.00%\n",
    "Cache Remaining%: 0.00%\n",
    "Xceivers: 1\n",
    "Last contact: Thu Dec 01 14:23:24 CST 2016\n",
    "\n",
    "\n",
    "Name: 172.17.8.10:50010 (wka01)\n",
    "Hostname: wka01\n",
    "Decommission Status : Normal\n",
    "Configured Capacity: 59050815488 (55.00 GB)\n",
    "DFS Used: 28672 (28 KB)\n",
    "Non DFS Used: 10941001728 (10.19 GB)\n",
    "DFS Remaining: 48109785088 (44.81 GB)\n",
    "DFS Used%: 0.00%\n",
    "DFS Remaining%: 81.47%\n",
    "Configured Cache Capacity: 0 (0 B)\n",
    "Cache Used: 0 (0 B)\n",
    "Cache Remaining: 0 (0 B)\n",
    "Cache Used%: 100.00%\n",
    "Cache Remaining%: 0.00%\n",
    "Xceivers: 1\n",
    "Last contact: Thu Dec 01 14:23:23 CST 2016\n",
    "\n",
    "\n",
    "Name: 172.17.8.12:50010 (wka03)\n",
    "Hostname: wka03\n",
    "Decommission Status : Normal\n",
    "Configured Capacity: 59050815488 (55.00 GB)\n",
    "DFS Used: 24576 (24 KB)\n",
    "Non DFS Used: 10941005824 (10.19 GB)\n",
    "DFS Remaining: 48109785088 (44.81 GB)\n",
    "DFS Used%: 0.00%\n",
    "DFS Remaining%: 81.47%\n",
    "Configured Cache Capacity: 0 (0 B)\n",
    "Cache Used: 0 (0 B)\n",
    "Cache Remaining: 0 (0 B)\n",
    "Cache Used%: 100.00%\n",
    "Cache Remaining%: 0.00%\n",
    "Xceivers: 1\n",
    "Last contact: Thu Dec 01 14:23:22 CST 2016\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metadata and hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I've been told that the metadate of the HDFS is stored at the name node. When we put a data to the HDFS, the file is probably split into blocks and stored on several data nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "bigred@CVBG:~/wk$ ssh nna tree nn\n",
    "nn\n",
    "├── current\n",
    "│   ├── edits_0000000000000000001-0000000000000000002\n",
    "│   ├── edits_0000000000000000003-0000000000000000003\n",
    "│   ├── edits_0000000000000000004-0000000000000000005\n",
    "│   ├── edits_0000000000000000006-0000000000000000007\n",
    "│   ├── edits_inprogress_0000000000000000008\n",
    "│   ├── fsimage_0000000000000000005\n",
    "│   ├── fsimage_0000000000000000005.md5\n",
    "│   ├── fsimage_0000000000000000007\n",
    "│   ├── fsimage_0000000000000000007.md5\n",
    "│   ├── seen_txid\n",
    "│   └── VERSION\n",
    "└── in_use.lock\n",
    "\n",
    "1 directory, 12 files\n",
    "bigred@CVBG:~/wk$ ssh wka01 tree dn\n",
    "dn\n",
    "├── current\n",
    "│   ├── BP-413870873-172.17.6.10-1480564977906\n",
    "│   │   ├── current\n",
    "│   │   │   ├── dfsUsed\n",
    "│   │   │   ├── finalized\n",
    "│   │   │   │   └── subdir0\n",
    "│   │   │   │       └── subdir0\n",
    "│   │   │   │           ├── blk_1073741825\n",
    "│   │   │   │           └── blk_1073741825_1001.meta\n",
    "│   │   │   ├── rbw\n",
    "│   │   │   └── VERSION\n",
    "│   │   ├── scanner.cursor\n",
    "│   │   └── tmp\n",
    "│   └── VERSION\n",
    "└── in_use.lock\n",
    "\n",
    "8 directories, 7 files\n",
    "bigred@CVBG:~/wk$ ssh wka02 tree dn\n",
    "dn\n",
    "├── current\n",
    "│   ├── BP-413870873-172.17.6.10-1480564977906\n",
    "│   │   ├── current\n",
    "│   │   │   ├── dfsUsed\n",
    "│   │   │   ├── finalized\n",
    "│   │   │   │   └── subdir0\n",
    "│   │   │   │       └── subdir0\n",
    "│   │   │   │           ├── blk_1073741825\n",
    "│   │   │   │           └── blk_1073741825_1001.meta\n",
    "│   │   │   ├── rbw\n",
    "│   │   │   └── VERSION\n",
    "│   │   ├── scanner.cursor\n",
    "│   │   └── tmp\n",
    "│   └── VERSION\n",
    "└── in_use.lock\n",
    "\n",
    "8 directories, 7 files\n",
    "bigred@CVBG:~/wk$ ssh wka03 tree dn\n",
    "dn\n",
    "├── current\n",
    "│   ├── BP-413870873-172.17.6.10-1480564977906\n",
    "│   │   ├── current\n",
    "│   │   │   ├── finalized\n",
    "│   │   │   ├── rbw\n",
    "│   │   │   └── VERSION\n",
    "│   │   ├── scanner.cursor\n",
    "│   │   └── tmp\n",
    "│   └── VERSION\n",
    "└── in_use.lock\n",
    "\n",
    "6 directories, 4 files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I've added one file to the HDFS. Now, I'm going to add another file to the HDFS and see how the data structure of the HDFS changes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "bigred@CVBG:~/wk$ hdfs dfs -put /etc/group /\n",
    "bigred@CVBG:~/wk$ ssh nna tree nn\n",
    "nn\n",
    "├── current\n",
    "│   ├── edits_0000000000000000001-0000000000000000002\n",
    "│   ├── edits_0000000000000000003-0000000000000000003\n",
    "│   ├── edits_0000000000000000004-0000000000000000005\n",
    "│   ├── edits_0000000000000000006-0000000000000000007\n",
    "│   ├── edits_inprogress_0000000000000000008\n",
    "│   ├── fsimage_0000000000000000005\n",
    "│   ├── fsimage_0000000000000000005.md5\n",
    "│   ├── fsimage_0000000000000000007\n",
    "│   ├── fsimage_0000000000000000007.md5\n",
    "│   ├── seen_txid\n",
    "│   └── VERSION\n",
    "└── in_use.lock\n",
    "\n",
    "1 directory, 12 files\n",
    "bigred@CVBG:~/wk$ ssh wka01 tree dn\n",
    "dn\n",
    "├── current\n",
    "│   ├── BP-413870873-172.17.6.10-1480564977906\n",
    "│   │   ├── current\n",
    "│   │   │   ├── dfsUsed\n",
    "│   │   │   ├── finalized\n",
    "│   │   │   │   └── subdir0\n",
    "│   │   │   │       └── subdir0\n",
    "│   │   │   │           ├── blk_1073741825\n",
    "│   │   │   │           └── blk_1073741825_1001.meta\n",
    "│   │   │   ├── rbw\n",
    "│   │   │   └── VERSION\n",
    "│   │   ├── scanner.cursor\n",
    "│   │   └── tmp\n",
    "│   └── VERSION\n",
    "└── in_use.lock\n",
    "\n",
    "8 directories, 7 files\n",
    "bigred@CVBG:~/wk$ ssh wka02 tree dn\n",
    "dn\n",
    "├── current\n",
    "│   ├── BP-413870873-172.17.6.10-1480564977906\n",
    "│   │   ├── current\n",
    "│   │   │   ├── dfsUsed\n",
    "│   │   │   ├── finalized\n",
    "│   │   │   │   └── subdir0\n",
    "│   │   │   │       └── subdir0\n",
    "│   │   │   │           ├── blk_1073741825\n",
    "│   │   │   │           ├── blk_1073741825_1001.meta\n",
    "│   │   │   │           ├── blk_1073741826\n",
    "│   │   │   │           └── blk_1073741826_1002.meta\n",
    "│   │   │   ├── rbw\n",
    "│   │   │   └── VERSION\n",
    "│   │   ├── scanner.cursor\n",
    "│   │   └── tmp\n",
    "│   └── VERSION\n",
    "└── in_use.lock\n",
    "\n",
    "8 directories, 9 files\n",
    "bigred@CVBG:~/wk$ ssh wka03 tree dn\n",
    "dn\n",
    "├── current\n",
    "│   ├── BP-413870873-172.17.6.10-1480564977906\n",
    "│   │   ├── current\n",
    "│   │   │   ├── finalized\n",
    "│   │   │   │   └── subdir0\n",
    "│   │   │   │       └── subdir0\n",
    "│   │   │   │           ├── blk_1073741826\n",
    "│   │   │   │           └── blk_1073741826_1002.meta\n",
    "│   │   │   ├── rbw\n",
    "│   │   │   └── VERSION\n",
    "│   │   ├── scanner.cursor\n",
    "│   │   └── tmp\n",
    "│   └── VERSION\n",
    "└── in_use.lock\n",
    "\n",
    "8 directories, 6 files\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### attempting to create the home directory for dsa01:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "bigred@CVBG:~/wk$ hdfs dfs -mkdir /usr\n",
    "bigred@CVBG:~/wk$ hdfs dfs -mkdir /usr/dsa01\n",
    "bigred@CVBG:~/wk$ hdfs dfs -chown dsa01:alpha /user/dsa01\n",
    "chown: `/user/dsa01': No such file or directory\n",
    "bigred@CVBG:~/wk$ hdfs dfs -mkdir /user\n",
    "bigred@CVBG:~/wk$ hdfs dfs -mkdir /user/dsa01\n",
    "bigred@CVBG:~/wk$ hdfs dfs -chown dsa01:alpha /user/dsa01\n",
    "bigred@CVBG:~/wk$ hdfs dfs -rm -r /usr\n",
    "16/12/01 16:07:24 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
    "Deleted /usr\n",
    "bigred@CVBG:~/wk$ ssh dsa01@cla01\n",
    "dsa01@cla01's password: \n",
    "Welcome to Ubuntu 14.04.5 LTS (GNU/Linux 4.4.0-47-generic x86_64)\n",
    "\n",
    " * Documentation:  https://help.ubuntu.com/\n",
    "Last login: Thu Dec  1 15:53:44 2016 from 172.17.0.1\n",
    "dsa01@cla01:~$ ls\n",
    "school.txt  u1_new.txt\n",
    "dsa01@cla01:~$ hdfs dfs -put school.txt school.txt \n",
    "hdsa01@cla01:~$ hdfs dfs -ls\n",
    "Found 1 items\n",
    "-rw-r--r--   2 dsa01 alpha      20807 2016-12-01 16:08 school.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mariadb.com/kb/en/mariadb/installing-and-using-mariadb-via-docker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 02122016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## short exam:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialization the cluster a:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "yes yes|dkcreate a && dkstart a && yes yes|formathdfs a jeff && starthdfs a && startyarn a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add users and set up the permission of the home directory of the users properly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "ssh nna 'sudo groupadd bigboss'\n",
    "ssh nna 'sudo useradd dsa00'\n",
    "ssh nna 'sudo useradd dsa01'\n",
    "ssh nna 'sudo usermod -aG bigboss dsa00'\n",
    "ssh nna 'sudo usermod -aG bigboss dsa01'\n",
    "\n",
    "hdfs dfs -mkdir -p /user/dsa00\n",
    "hdfs dfs -mkdir -p /user/dsa01\n",
    "\n",
    "hdfs dfs -chown dsa00:bigboss /user/dsa00\n",
    "hdfs dfs -chown dsa01:bigboss /user/dsa01\n",
    "hdfs dfs -chmod 770 /user/dsa00\n",
    "hdfs dfs -chmod 770 /user/dsa01\n",
    "hdfs dfs -chmod -R 777 /tmp/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### stop and remove the cluster a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "#!/bin/bash\n",
    "stopyarn a && stophdfs a && dkstop a && yes yes|dkremove a\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "## entering into pig\n",
    "a= load 'school.txt';\n",
    "b = foreach a generate $0,$1;\n",
    "c = filter b by $0 is not null;\n",
    "dump c;\n",
    "store c into 'abc.csv';\n",
    "## now, exiting pig\n",
    "hdfs dfs -getmerge abc.csv abc.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yarn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "bigred@CVBG:~/wk$ startyarn a\n",
    "Cluster Type = small\n",
    "starting resourcemanager, logging to /tmp/yarn-bigred-resourcemanager-rma.out\n",
    "starting historyserver, logging to /home/bigred/jhslog/mapred-bigred-historyserver-rma.out\n",
    "starting nodemanager, logging to /tmp/yarn-bigred-nodemanager-wka01.out\n",
    "starting nodemanager, logging to /tmp/yarn-bigred-nodemanager-wka02.out\n",
    "bigred@CVBG:~/wk$ ssh rma jps  // go to the resource manager and \n",
    "225 JobHistoryServer\n",
    "151 ResourceManager\n",
    "490 Jps\n",
    "bigred@CVBG:~/wk$ yarn node -list -all\n",
    "16/12/02 13:51:41 INFO client.RMProxy: Connecting to ResourceManager at rma/172.17.6.12:8032\n",
    "16/12/02 13:51:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Total Nodes:2\n",
    "         Node-Id\t     Node-State\tNode-Http-Address\tNumber-of-Running-Containers\n",
    "     wka01:32886\t        RUNNING\t       wka01:8042\t                           0\n",
    "     wka02:42447\t        RUNNING\t       wka02:8042\t\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### run a test example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop jar /opt/hadoop-2.7.3/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar pi 2 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "dsa01@cla01:~$ pig 2> /dev/null\n",
    "grunt> pwd\n",
    "hdfs://nna:8020/user/dsa01\n",
    "grunt> ls\n",
    "hdfs://nna:8020/user/dsa01/QuasiMonteCarlo_1480660609872_1406370602\t<dir>\n",
    "hdfs://nna:8020/user/dsa01/a1<r 2>\t0\n",
    "hdfs://nna:8020/user/dsa01/a2<r 2>\t0\n",
    "hdfs://nna:8020/user/dsa01/school.txt<r 2>\t20807\n",
    "hdfs://nna:8020/user/dsa01/temp\t<dir>\n",
    "hdfs://nna:8020/user/dsa01/testtest\t<dir>\n",
    "grunt> mkdir test\n",
    "grunt> \\rm -r test\n",
    "grunt> ls\n",
    "hdfs://nna:8020/user/dsa01/QuasiMonteCarlo_1480660609872_1406370602\t<dir>\n",
    "hdfs://nna:8020/user/dsa01/a1<r 2>\t0\n",
    "hdfs://nna:8020/user/dsa01/a2<r 2>\t0\n",
    "hdfs://nna:8020/user/dsa01/school.txt<r 2>\t20807\n",
    "hdfs://nna:8020/user/dsa01/temp\t<dir>\n",
    "hdfs://nna:8020/user/dsa01/test\t<dir>\n",
    "hdfs://nna:8020/user/dsa01/testtest\t<dir>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "grunt> a= load 'school.txt';\n",
    "grunt> b = foreach a generate $0,$1;\n",
    "grunt> c = filter b by $0 is not null;\n",
    "grunt> dump c;\n",
    "grunt> store c into 'abc.csv';\n",
    "\n",
    "$hdfs dfs -getmerge abc.csv abc.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
